FROM ubuntu:22.04

## Spark version
ENV SPARK_VERSION=3.5.5

COPY ./config/python/requirements.txt /requirements.txt
COPY ./config/spark/spark.entrypoint.sh /spark.entrypoint.sh

RUN apt-get update \
    && apt-get install -y wget openjdk-11-jdk ssh rsync curl \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

ENV PYTHON_VERSION=3.11.4
ENV CONDA_HOME=/opt/miniconda
ENV PATH=${CONDA_HOME}/bin:$PATH
RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -O /tmp/miniconda.sh && \
    bash /tmp/miniconda.sh -b -p ${CONDA_HOME} && \
    rm /tmp/miniconda.sh


RUN conda init --all && \
    . ~/.bashrc && \
    conda update -n base -c defaults conda -y && \
    conda create --name spark_env python=${PYTHON_VERSION} && \
    conda activate spark_env && \
    pip install -r /requirements.txt 



# Install Spark
ENV SPARK_VERSION=3.5.5
ENV SPARK_HOME=/opt/spark
ENV PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH
RUN wget --quiet "https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}.tgz" -O /tmp/spark.tgz \
    && tar -xzf /tmp/spark.tgz -C /opt \
    && rm /tmp/spark.tgz \
    && mv /opt/spark-${SPARK_VERSION} ${SPARK_HOME}

ENV SPARK_MASTER_URL=spark://spark-master:7077
RUN chmod +x /spark.entrypoint.sh
ENTRYPOINT ["/spark.entrypoint.sh"]