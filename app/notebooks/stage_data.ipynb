{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e5ae7c-2131-4c1d-8c19-e548bb6f3861",
   "metadata": {},
   "source": [
    "### To-Do List\n",
    "\n",
    "- [ ] Read data from repo\n",
    "- [ ] split data with different seeds\n",
    "- [ ] save each train and test for each seed in different folder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023c906e-49ed-4bd9-9e2e-ecd97f33aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "import os\n",
    "from utils.schemas import raw_data_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0320ff6d-a973-4dbe-8210-8777b4b64bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/04 06:37:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#train_path = os.environ.get(\"TRAIN_PATH\")\n",
    "#test_path = os.environ.get(\"TEST_PATH\")\n",
    "spark_master = os.environ.get(\"SPARK_MASTER_URL\")\n",
    "hadoop_master = os.environ.get(\"HADOOP_MASTER_URL\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(spark_master) \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", hadoop_master) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde6c709-9fb4-430e-938f-27c143cf367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: 30000 rows\n",
      "Targets shape: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 30000 rows\n"
     ]
    }
   ],
   "source": [
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "X = spark.createDataFrame(default_of_credit_card_clients.data.features)\n",
    "y = spark.createDataFrame(default_of_credit_card_clients.data.targets)\n",
    "\n",
    "# Print initial data info\n",
    "print(f\"Features shape: {X.count()} rows\")\n",
    "print(f\"Targets shape: {y.count()} rows\")\n",
    "\n",
    "# combine X and y\n",
    "X = X.withColumn(\"id\", monotonically_increasing_id())\n",
    "y = y.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = X.join(y, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "\n",
    "total_count = df.count()\n",
    "print(f\"Combined dataset size: {total_count} rows\")\n",
    "\n",
    "train, test = df.randomSplit([.7,.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f492bf-0a90-4f39-8246-07c424d5a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#df.write.mode('overwrite').options(header='True', delimiter=',').parquet('/mnt/train')\n",
    "#df.write.mode('overwrite').options(header='True', delimiter=',').csv('/mnt/train.csv')\n",
    "#test.write.options(header='True', delimiter=',').csv(test_path)\n",
    "train.write.mode(\"overwrite\").parquet(\"data/train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c8f85f-f1f9-4d0f-91e7-2e7b4054fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train.coalesce(1).write.mode(\"overwrite\") \\\n",
    "        #.option(\"schema\", raw_data_schema.json()) \\\n",
    "        .parquet(\"data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83acd209-b834-4399-8686-ca758584cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_read = spark.read.schema(raw_data_schema).parquet('data/train')\n",
    "#test_read = spark.read.schema(raw_data_schema).csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbde0944-4973-40bb-b81d-332d0e1e2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "21063"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_read.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88ec504-b178-4a7f-a0e7-805510e9bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d340ee43-2847-416c-813f-7fec4bea05f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyarrow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpyarrow\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyarrow' is not defined"
     ]
    }
   ],
   "source": [
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbda0c1e-cfa2-4ebf-929b-502b522f8e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_aarch64.whl (40.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aad46a-2ac7-4958-a3ec-91aa233f2517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073b4c0-57bc-4320-9235-1607bde2f8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2be2a3-258e-4085-b04f-9173a946831c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabcfa8-da61-434d-b4ad-89b88f237a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to absolute paths:\n",
      "Train: /app/data/train\n",
      "Test: /app/data/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets shape: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes - Train: 21063, Test: 8937\n",
      "root\n",
      " |-- X1: long (nullable = true)\n",
      " |-- X2: long (nullable = true)\n",
      " |-- X3: long (nullable = true)\n",
      " |-- X4: long (nullable = true)\n",
      " |-- X5: long (nullable = true)\n",
      " |-- X6: long (nullable = true)\n",
      " |-- X7: long (nullable = true)\n",
      " |-- X8: long (nullable = true)\n",
      " |-- X9: long (nullable = true)\n",
      " |-- X10: long (nullable = true)\n",
      " |-- X11: long (nullable = true)\n",
      " |-- X12: long (nullable = true)\n",
      " |-- X13: long (nullable = true)\n",
      " |-- X14: long (nullable = true)\n",
      " |-- X15: long (nullable = true)\n",
      " |-- X16: long (nullable = true)\n",
      " |-- X17: long (nullable = true)\n",
      " |-- X18: long (nullable = true)\n",
      " |-- X19: long (nullable = true)\n",
      " |-- X20: long (nullable = true)\n",
      " |-- X21: long (nullable = true)\n",
      " |-- X22: long (nullable = true)\n",
      " |-- X23: long (nullable = true)\n",
      " |-- Y: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- X1: long (nullable = true)\n",
      " |-- X2: long (nullable = true)\n",
      " |-- X3: long (nullable = true)\n",
      " |-- X4: long (nullable = true)\n",
      " |-- X5: long (nullable = true)\n",
      " |-- X6: long (nullable = true)\n",
      " |-- X7: long (nullable = true)\n",
      " |-- X8: long (nullable = true)\n",
      " |-- X9: long (nullable = true)\n",
      " |-- X10: long (nullable = true)\n",
      " |-- X11: long (nullable = true)\n",
      " |-- X12: long (nullable = true)\n",
      " |-- X13: long (nullable = true)\n",
      " |-- X14: long (nullable = true)\n",
      " |-- X15: long (nullable = true)\n",
      " |-- X16: long (nullable = true)\n",
      " |-- X17: long (nullable = true)\n",
      " |-- X18: long (nullable = true)\n",
      " |-- X19: long (nullable = true)\n",
      " |-- X20: long (nullable = true)\n",
      " |-- X21: long (nullable = true)\n",
      " |-- X22: long (nullable = true)\n",
      " |-- X23: long (nullable = true)\n",
      " |-- Y: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully written to disk\n",
      "Train directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "write_data(train_path, test_path, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0601d82-44e0-4a7e-8ca6-2fb6aef6e3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read from:\n",
      "Train: /app/data/train\n",
      "Test: /app/data/test\n",
      "Train directory contents: ['._SUCCESS.crc', '_SUCCESS']\n",
      "Test directory contents: ['._SUCCESS.crc', '_SUCCESS']\n",
      "Schema of loaded train data:\n",
      "root\n",
      " |-- X1: long (nullable = true)\n",
      " |-- X2: long (nullable = true)\n",
      " |-- X3: long (nullable = true)\n",
      " |-- X4: long (nullable = true)\n",
      " |-- X5: long (nullable = true)\n",
      " |-- X6: long (nullable = true)\n",
      " |-- X7: long (nullable = true)\n",
      " |-- X8: long (nullable = true)\n",
      " |-- X9: long (nullable = true)\n",
      " |-- X10: long (nullable = true)\n",
      " |-- X11: long (nullable = true)\n",
      " |-- X12: long (nullable = true)\n",
      " |-- X13: long (nullable = true)\n",
      " |-- X14: long (nullable = true)\n",
      " |-- X15: long (nullable = true)\n",
      " |-- X16: long (nullable = true)\n",
      " |-- X17: long (nullable = true)\n",
      " |-- X18: long (nullable = true)\n",
      " |-- X19: long (nullable = true)\n",
      " |-- X20: long (nullable = true)\n",
      " |-- X21: long (nullable = true)\n",
      " |-- X22: long (nullable = true)\n",
      " |-- X23: long (nullable = true)\n",
      " |-- Y: long (nullable = true)\n",
      "\n",
      "Initial read - Train count: 0, Test count: 0\n",
      "Error reading parquet files: Read data is empty\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Read data is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(train_path, test_path, spark)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial read - Train count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m test_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead data is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain data read: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest data read: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Read data is empty"
     ]
    }
   ],
   "source": [
    "train, test = read_data(train_path, test_path, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec712c04-3e9c-44c2-99e9-dc3f07e41f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(train_path, test_path, spark):\n",
    "    # Ensure absolute paths\n",
    "    train_path = os.path.abspath(train_path)\n",
    "    test_path = os.path.abspath(test_path)\n",
    "    \n",
    "    print(f\"Writing to absolute paths:\\nTrain: {train_path}\\nTest: {test_path}\")\n",
    "    \n",
    "    # fetch dataset \n",
    "    default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "    X = spark.createDataFrame(default_of_credit_card_clients.data.features)\n",
    "    y = spark.createDataFrame(default_of_credit_card_clients.data.targets)\n",
    "\n",
    "    # Print initial data info\n",
    "    print(f\"Features shape: {X.count()} rows\")\n",
    "    print(f\"Targets shape: {y.count()} rows\")\n",
    "\n",
    "    # combine X and y\n",
    "    X = X.withColumn(\"id\", monotonically_increasing_id())\n",
    "    y = y.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df = X.join(y, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "    \n",
    "    total_count = df.count()\n",
    "    print(f\"Combined dataset size: {total_count} rows\")\n",
    "\n",
    "    train, test = df.randomSplit([.7,.3], seed=42)\n",
    "    \n",
    "    # Get counts before writing\n",
    "    train_count = train.count()\n",
    "    test_count = test.count()\n",
    "    print(f\"Split sizes - Train: {train_count}, Test: {test_count}\")\n",
    "\n",
    "    if train_count == 0 or test_count == 0:\n",
    "        raise ValueError(\"Train or test dataset is empty after split\")\n",
    "        \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(train_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(test_path), exist_ok=True)\n",
    "\n",
    "    train.printSchema()\n",
    "    test.printSchema()\n",
    "    \n",
    "    # Write with coalesce instead of repartition for smaller files\n",
    "    train.coalesce(1).write.mode(\"overwrite\").parquet(train_path)\n",
    "    test.coalesce(1).write.mode(\"overwrite\").parquet(test_path)\n",
    "    \n",
    "    # Verify files exist after writing\n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(\"Failed to write parquet files\")\n",
    "        \n",
    "    print(f\"Files successfully written to disk\")\n",
    "    print(f\"Train directory exists: {os.path.exists(train_path)}\")\n",
    "    print(f\"Test directory exists: {os.path.exists(test_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af441e2-242f-44cb-9d6f-38196192ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_path, test_path, spark):\n",
    "    print(f\"Attempting to read from:\\nTrain: {train_path}\\nTest: {test_path}\")\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"Train path does not exist: {train_path}\")\n",
    "    if not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(f\"Test path does not exist: {test_path}\")\n",
    "        \n",
    "    # List contents of directories\n",
    "    print(f\"Train directory contents: {os.listdir(train_path)}\")\n",
    "    print(f\"Test directory contents: {os.listdir(test_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Read with predefined schema\n",
    "        train = spark.read.schema(raw_data_schema).parquet(train_path)\n",
    "        test = spark.read.schema(raw_data_schema).parquet(test_path)\n",
    "        \n",
    "        print(\"Schema of loaded train data:\")\n",
    "        train.printSchema()\n",
    "        \n",
    "        # Verify the data was read successfully\n",
    "        train_count = train.count()\n",
    "        test_count = test.count()\n",
    "        \n",
    "        print(f\"Initial read - Train count: {train_count}, Test count: {test_count}\")\n",
    "        \n",
    "        if train_count == 0 or test_count == 0:\n",
    "            raise ValueError(\"Read data is empty\")\n",
    "            \n",
    "        print(f\"Train data read: {train_count} rows\")\n",
    "        print(f\"Test data read: {test_count} rows\")\n",
    "        \n",
    "        return train, test\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet files: {str(e)}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
