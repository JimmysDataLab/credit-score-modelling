{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72186d0-b465-409a-9bf6-4b1ab6f17025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca51783-2e0e-47e6-8dd6-9afddb06f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d3aae9-4f1a-4c4b-ba90-c0b8d68a71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9\n",
    "spark_master = os.environ.get(\"SPARK_MASTER_URL\")\n",
    "train_path = os.path.join(os.environ.get(\"TRAIN_PATH\"))\n",
    "test_path = os.path.join(os.environ.get(\"TEST_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732f0cef-39b3-42c9-a9f3-a1da7cd9336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark session\n",
    "spark_master = os.environ.get(\"SPARK_MASTER_URL\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Random-Forest-Classifier\") \\\n",
    "    .master(spark_master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger = logging.getLogger(\"py4j\")\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff32f7a2-9f60-44c0-9482-df69b4102f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.stage_data import write_data, read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110ad706-1860-4b95-aa28-507a9b08ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to absolute paths:\n",
      "Train: /app/data/train\n",
      "Test: /app/data/test\n",
      "Features shape: 30000 rows\n",
      "Targets shape: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 30000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes - Train: 21063, Test: 8937\n",
      "root\n",
      " |-- X1: long (nullable = true)\n",
      " |-- X2: long (nullable = true)\n",
      " |-- X3: long (nullable = true)\n",
      " |-- X4: long (nullable = true)\n",
      " |-- X5: long (nullable = true)\n",
      " |-- X6: long (nullable = true)\n",
      " |-- X7: long (nullable = true)\n",
      " |-- X8: long (nullable = true)\n",
      " |-- X9: long (nullable = true)\n",
      " |-- X10: long (nullable = true)\n",
      " |-- X11: long (nullable = true)\n",
      " |-- X12: long (nullable = true)\n",
      " |-- X13: long (nullable = true)\n",
      " |-- X14: long (nullable = true)\n",
      " |-- X15: long (nullable = true)\n",
      " |-- X16: long (nullable = true)\n",
      " |-- X17: long (nullable = true)\n",
      " |-- X18: long (nullable = true)\n",
      " |-- X19: long (nullable = true)\n",
      " |-- X20: long (nullable = true)\n",
      " |-- X21: long (nullable = true)\n",
      " |-- X22: long (nullable = true)\n",
      " |-- X23: long (nullable = true)\n",
      " |-- Y: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- X1: long (nullable = true)\n",
      " |-- X2: long (nullable = true)\n",
      " |-- X3: long (nullable = true)\n",
      " |-- X4: long (nullable = true)\n",
      " |-- X5: long (nullable = true)\n",
      " |-- X6: long (nullable = true)\n",
      " |-- X7: long (nullable = true)\n",
      " |-- X8: long (nullable = true)\n",
      " |-- X9: long (nullable = true)\n",
      " |-- X10: long (nullable = true)\n",
      " |-- X11: long (nullable = true)\n",
      " |-- X12: long (nullable = true)\n",
      " |-- X13: long (nullable = true)\n",
      " |-- X14: long (nullable = true)\n",
      " |-- X15: long (nullable = true)\n",
      " |-- X16: long (nullable = true)\n",
      " |-- X17: long (nullable = true)\n",
      " |-- X18: long (nullable = true)\n",
      " |-- X19: long (nullable = true)\n",
      " |-- X20: long (nullable = true)\n",
      " |-- X21: long (nullable = true)\n",
      " |-- X22: long (nullable = true)\n",
      " |-- X23: long (nullable = true)\n",
      " |-- Y: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully written to disk\n",
      "Train directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "write_data(train_path,test_path,spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab792f6-80d1-4df2-8f38-262fc06daf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read from:\n",
      "Train: /app/data/train\n",
      "Test: /app/data/test\n",
      "Train directory contents: ['._SUCCESS.crc', '_SUCCESS']\n",
      "Test directory contents: ['._SUCCESS.crc', '_SUCCESS']\n",
      "Error reading parquet files: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/scripts/stage_data.py:77\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(train_path, test_path, spark)\u001b[0m\n\u001b[1;32m     73\u001b[0m print(f\"Train directory contents: {os.listdir(train_path)}\")\n\u001b[1;32m     74\u001b[0m print(f\"Test directory contents: {os.listdir(test_path)}\")\n\u001b[1;32m     76\u001b[0m try:\n\u001b[0;32m---> 77\u001b[0m     # Read with predefined schema\n\u001b[1;32m     78\u001b[0m     train = spark.read.schema(raw_data_schema).parquet(train_path)\n\u001b[1;32m     79\u001b[0m     test = spark.read.schema(raw_data_schema).parquet(test_path)\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "train, test = read_data(train_path,test_path,spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ac745-6315-4fc9-9fae-41ed7b02ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fetch dataset \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "X = spark.createDataFrame(default_of_credit_card_clients.data.features)\n",
    "y = spark.createDataFrame(default_of_credit_card_clients.data.targets)\n",
    "\n",
    "# combine X and y\n",
    "X = X.withColumn(\"id\", monotonically_increasing_id())\n",
    "y = y.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = X.join(y, on=\"id\", how=\"inner\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddcd6b-4758-40d5-b636-6535b72dc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [1,2,4,56,42,75,12,56,9]\n",
    "for seed in SEEDS:\n",
    "\n",
    "    train, test = df.randomSplit([.7,.3], seed=seed)\n",
    "    train.write.mode(\"overwrite\").parquet(os.path.join(train_path,str(seed)))\n",
    "    test.write.mode(\"overwrite\").parquet(os.path.join(test_path,str(seed)))\n",
    "    print(train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7534b841-babe-4b09-9d4e-ab766655f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"X1\", LongType(), True),\n",
    "    StructField(\"X2\", LongType(), True),\n",
    "    StructField(\"X3\", LongType(), True),\n",
    "    StructField(\"X4\", LongType(), True),\n",
    "    StructField(\"X5\", LongType(), True),\n",
    "    StructField(\"X6\", LongType(), True),\n",
    "    StructField(\"X7\", LongType(), True),\n",
    "    StructField(\"X8\", LongType(), True),\n",
    "    StructField(\"X9\", LongType(), True),\n",
    "    StructField(\"X10\", LongType(), True),\n",
    "    StructField(\"X11\", LongType(), True),\n",
    "    StructField(\"X12\", LongType(), True),\n",
    "    StructField(\"X13\", LongType(), True),\n",
    "    StructField(\"X14\", LongType(), True),\n",
    "    StructField(\"X15\", LongType(), True),\n",
    "    StructField(\"X16\", LongType(), True),\n",
    "    StructField(\"X17\", LongType(), True),\n",
    "    StructField(\"X18\", LongType(), True),\n",
    "    StructField(\"X19\", LongType(), True),\n",
    "    StructField(\"X20\", LongType(), True),\n",
    "    StructField(\"X21\", LongType(), True),\n",
    "    StructField(\"X22\", LongType(), True),\n",
    "    StructField(\"X23\", LongType(), True),\n",
    "    StructField(\"Y\", LongType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bdc04f-e676-4906-99ba-37284556e3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the DataFrame from the saved Parquet file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#test = spark.read.schema(schema).parquet(os.path.join(train_path,'9'))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda/envs/spark_env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "# Read the DataFrame from the saved Parquet file\n",
    "\n",
    "train = spark.read.parquet(train_path)\n",
    "#test = spark.read.schema(schema).parquet(os.path.join(train_path,'9'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b84e10-a772-4e00-a12c-9ac00b30294a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ae2d-d04d-4805-8b36-3b81dafeebbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b760344-ade3-4b49-91c6-1f2d9b3f9fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8b336-6583-4820-b84d-4f18ad4be03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1600a7f0-4e53-41fd-b7d6-e7e4c8cb898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a0f03-fc9c-455f-a842-412e02ef0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cf5a6-46be-48e3-b34a-4cf6f322f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"file///\",train_path,'98')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f991169-b2c3-4c78-9c1f-bf29230833eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_fs = spark.sparkContext.getConf().get(\"fs.defaultFS\")\n",
    "print(\"Default File System:\", default_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85334c-8d4a-4709-9e6a-08e959136ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384f1b9-8f0d-4f9b-9fcb-e6c4b9d688e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(train_path,str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b032e-648a-451f-8886-ab5e2345afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numerical and categorical columns\n",
    "target = ['Y']\n",
    "num_feat = ['X1','X5','X12','X13','X14','X15','X16','X17','X18','X19','X20','X21','X22','X23']\n",
    "cat_feat = [col for col in train.columns if col not in num_feat+target]\n",
    "cat_feat_indexed = [f\"{col}_i\" for col in cat_feat]\n",
    "cat_feat_encoded = [f\"{col}_e\" for col in cat_feat_indexed]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724ac74-0c96-4b76-b68c-d196a89bbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Transformations\n",
    "string_indexer = StringIndexer(inputCols=cat_feat, outputCols=cat_feat_indexed, handleInvalid='keep')\n",
    "hot_encoder = OneHotEncoder(inputCols=cat_feat_indexed, outputCols=cat_feat_encoded, handleInvalid='keep')\n",
    "vector_assembler_1 = VectorAssembler(inputCols=num_feat+cat_feat_encoded,outputCol=\"features\")\n",
    "\n",
    "# Target Transformations\n",
    "string_indexer_target = StringIndexer(inputCol='Y',outputCol='Y_i',handleInvalid='keep')\n",
    "hot_encoder_target = OneHotEncoder(inputCol='Y_i', outputCol='Y_i_e',handleInvalid='keep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403c21c-a5eb-400d-8644-c6c18cd3bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Indexing (might be better for trees)\n",
    "vector_assembler_2 = VectorAssembler(inputCols=num_feat+cat_feat,\\\n",
    "                                     outputCol=\"features\")\n",
    "vector_indexer = VectorIndexer(maxCategories=15,inputCol=\"features\",\\\n",
    "                               outputCol=\"indexed_features\")\n",
    "\n",
    "# Target\n",
    "vector_indexer_target = VectorIndexer(maxCategories=15,inputCol='Y',\\\n",
    "                               outputCol=\"indexed_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad25b8-af69-4c3d-83f9-05809e1557cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline1 = Pipeline(stages=[string_indexer, \\\n",
    "                            hot_encoder, \\\n",
    "                            vector_assembler_1, \\\n",
    "                            string_indexer_target, \\\n",
    "                            hot_encoder_target])\n",
    "train_1 = pipeline1.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35bf85-8e20-4308-b440-b6247d581936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline2 = Pipeline(stages=[vector_assembler_2,\\\n",
    "                            vector_indexer,\\\n",
    "                            vector_indexer_target])\n",
    "train_2 = pipeline2.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df414458-9be0-4802-a048-4fc8d52af193",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
